import os
import re
from typing import List, Dict, Any, Tuple, Optional

import streamlit as st
from dotenv import load_dotenv

from langchain.chat_models import init_chat_model
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.documents import Document

from langchain_ollama import OllamaEmbeddings
from langchain_chroma import Chroma

# =========================
# åŸºç¡€é…ç½®
# =========================
load_dotenv()

st.set_page_config(page_title="Medical RAG Demo", page_icon="ğŸ©º", layout="wide")


# =========================
# ä½ çš„æ¨¡å‹ / Embedding / å‘é‡åº“åˆå§‹åŒ–æ–¹å¼ï¼ˆä¸¥æ ¼æŒ‰ä½ çš„å†™æ³•ï¼‰
# =========================
@st.cache_resource
def load_models_and_store():
    model = init_chat_model(
        model="ollama:deepseek-r1:8b",
        base_url="http://localhost:11434",
        temperature=0.1,
    )

    embedding = OllamaEmbeddings(
        model="qwen3-embedding:4b",
        base_url="http://localhost:11434",
    )

    vector_store = Chroma(
        collection_name="medical_rag_collection",
        embedding_function=embedding,
        persist_directory="./chroma_rag_db"
    )

    return model, embedding, vector_store


model, embedding, vector_store = load_models_and_store()

# =========================
# Promptsï¼ˆæ‹’ç»ä¸ç¡®å®š + å¼•ç”¨æ¥æº + å¤šè½®ï¼‰
# =========================

# 1) å¯¹è¯æ‘˜è¦ï¼ˆé•¿å¯¹è¯ç”¨ï¼‰
SUMMARY_SYSTEM = """ä½ æ˜¯ä¸€ä¸ªå¯¹è¯è®°å½•å‹ç¼©å™¨ã€‚
å°†å¯¹è¯å†å²å‹ç¼©æˆâ€œäº‹å®æ‘˜è¦â€ï¼Œä¿ç•™ï¼šç”¨æˆ·ç—‡çŠ¶/æ—¶é—´çº¿/å…³é”®æ£€æŸ¥/å…³é”®ç»“è®º/æœªè§£å†³é—®é¢˜/é‡è¦é™åˆ¶æ¡ä»¶ã€‚
ä¸è¦ç¼–é€ ã€‚è¾“å‡ºä¸­æ–‡çº¯æ–‡æœ¬ï¼Œå°½é‡çŸ­ï¼ˆ<= 600å­—ï¼‰ã€‚"""

summary_prompt = ChatPromptTemplate.from_messages([
    ("system", SUMMARY_SYSTEM),
    ("human", "å·²æœ‰æ‘˜è¦ï¼š\n{summary}\n\næ–°å¢å¯¹è¯ç‰‡æ®µï¼š\n{new_lines}\n\nè¯·è¾“å‡ºæ›´æ–°åçš„æ‘˜è¦ï¼š")
])

# 2) æ£€ç´¢ query é‡å†™ï¼ˆæ›´é€‚åˆå¤šè½®ï¼‰
REWRITE_SYSTEM = """ä½ æ˜¯ä¸€ä¸ªæŸ¥è¯¢æ”¹å†™å™¨ã€‚
ç»™å®šå¯¹è¯æ‘˜è¦ + æœ€è¿‘å¯¹è¯ï¼ŒæŠŠç”¨æˆ·æœ€æ–°é—®é¢˜æ”¹å†™æˆâ€œç‹¬ç«‹ã€å¯æ£€ç´¢â€çš„ä¸­æ–‡æŸ¥è¯¢ï¼ˆåªè¾“å‡ºä¸€è¡ŒæŸ¥è¯¢ï¼Œä¸è¦è§£é‡Šï¼‰ã€‚"""

rewrite_prompt = ChatPromptTemplate.from_messages([
    ("system", REWRITE_SYSTEM),
    ("human", "å¯¹è¯æ‘˜è¦ï¼š{summary}\n\næœ€è¿‘å¯¹è¯ï¼š\n{recent}\n\nç”¨æˆ·æœ€æ–°é—®é¢˜ï¼š{question}\n\nç‹¬ç«‹æ£€ç´¢æŸ¥è¯¢ï¼š")
])

# 3) RAG å›ç­”ï¼ˆæ‹’ç­” + å¼•ç”¨ï¼‰
ANSWER_SYSTEM = """ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„ä¸­æ–‡åŒ»ç–—é—®ç­”åŠ©æ‰‹ï¼ˆRAGï¼‰ã€‚
ä½ ä¼šæ”¶åˆ°ï¼šå¯¹è¯æ‘˜è¦ã€æœ€è¿‘å¯¹è¯ã€ä»¥åŠæ£€ç´¢èµ„æ–™(context)ã€‚
è¦æ±‚ï¼š
1) åªèƒ½ä¾æ® context ä¸­çš„ä¿¡æ¯å›ç­”ï¼›å¦‚æœ context ä¸è¶³ä»¥æ”¯æŒç»“è®ºï¼Œå¿…é¡»æ˜ç¡®è¯´â€œèµ„æ–™ä¸è¶³/ä¸ç¡®å®šâ€ï¼Œå¹¶ç»™å‡ºä¿å®ˆå»ºè®®ï¼ˆå¦‚å»ºè®®å°±åŒ»ç§‘å®¤ã€å±é™©ä¿¡å·ã€éœ€è¦è¡¥å……çš„ä¿¡æ¯ï¼‰ã€‚
2) ä¸¥ç¦ç¼–é€ ï¼šä¸è¦ç»™å‡º context ä¸­æ²¡æœ‰çš„å…³é”®äº‹å®ï¼ˆå°¤å…¶æ˜¯ç¡®è¯Šç»“è®ºã€å…·ä½“è¯åå‰‚é‡ã€æ£€æŸ¥ç»“æœï¼‰ã€‚
3) å¿…é¡»è¾“å‡º Markdownï¼Œå¹¶åŒ…å«ä¸¤ä¸ªéƒ¨åˆ†ï¼š
   - ã€å›ç­”ã€‘...ï¼ˆé¢å‘ç”¨æˆ·ï¼‰
   - ã€å¼•ç”¨æ¥æºã€‘åˆ—å‡ºä½ ç”¨åˆ°çš„èµ„æ–™æ¡ç›®ç¼–å·ï¼ˆå¦‚ 1ã€2ï¼‰ä»¥åŠå¯¹åº”çš„ answer_idï¼ˆæ¥è‡ªèµ„æ–™å¤´éƒ¨ answer_id=...ï¼‰
"""

answer_prompt = ChatPromptTemplate.from_messages([
    ("system", ANSWER_SYSTEM),
    ("human",
     "å¯¹è¯æ‘˜è¦ï¼š\n{summary}\n\n"
     "æœ€è¿‘å¯¹è¯ï¼š\n{recent}\n\n"
     "ç”¨æˆ·é—®é¢˜ï¼š{question}\n\n"
     "æ£€ç´¢èµ„æ–™(context)ï¼š\n{context}\n\n"
     "è¯·å›ç­”ï¼š")
])


# =========================
# å·¥å…·å‡½æ•°
# =========================
def format_recent_messages(messages: List[Dict[str, str]], max_turns: int = 8) -> str:
    """
    ä¿ç•™æœ€è¿‘ max_turns è½®ï¼ˆuser+assistant ç®—ä¸€è½®ï¼‰åŸæ–‡ï¼Œç”¨äºå¢å¼ºè¿è´¯æ€§ã€‚
    """
    # messages: [{"role": "user"/"assistant", "content": "..."}]
    if not messages:
        return ""
    # å–æœ€å 2*max_turns æ¡æ¶ˆæ¯
    tail = messages[-2 * max_turns:]
    lines = []
    for m in tail:
        role = "ç”¨æˆ·" if m["role"] == "user" else "åŠ©æ‰‹"
        lines.append(f"{role}ï¼š{m['content']}")
    return "\n".join(lines)


def docs_to_context_with_ids(docs_with_scores: List[Tuple[Document, float]], max_chars: int = 4500) -> str:
    """
    æŠŠæ£€ç´¢èµ„æ–™æ‹¼æˆ contextï¼Œå¹¶æ˜¾å¼æ ‡å· + answer_idï¼Œæ–¹ä¾¿â€œå¼•ç”¨æ¥æºâ€è¾“å‡ºã€‚
    score ä»…ç”¨äºå±•ç¤ºï¼Œä¸è¦æ±‚æ¨¡å‹ç†è§£ã€‚
    """
    blocks = []
    total = 0
    for idx, (doc, score) in enumerate(docs_with_scores, start=1):
        meta = doc.metadata or {}
        ans_id = meta.get("answer_id", None)
        qid = meta.get("question_id", None)
        chunk_id = meta.get("chunk_id", None)
        header = f"[èµ„æ–™{idx}] answer_id={ans_id} question_id={qid} chunk_id={chunk_id} score={score:.4f}"
        body = (doc.page_content or "").strip()
        block = f"{header}\n{body}\n"
        if total + len(block) > max_chars:
            break
        blocks.append(block)
        total += len(block)
    return "\n".join(blocks).strip()


def extract_used_source_nums(answer_md: str) -> List[int]:
    """
    ä»ã€å¼•ç”¨æ¥æºã€‘é‡Œç²—ç•¥æŠ½å–â€œèµ„æ–™ç¼–å·â€ï¼ˆ1ã€2ã€3...ï¼‰ã€‚
    ä¸æ˜¯å¿…é¡»å‡†ç¡®ï¼Œåªç”¨äº UI é«˜äº®ï¼ˆå¤±è´¥ä¹Ÿæ— æ‰€è°“ï¼‰ã€‚
    """
    if not answer_md:
        return []
    # åŒ¹é…â€œèµ„æ–™1 / 1 / 1ã€2â€ç­‰
    nums = re.findall(r"(?:èµ„æ–™)?\s*([1-9]\d*)", answer_md)
    out = []
    for n in nums:
        try:
            out.append(int(n))
        except:
            pass
    # å»é‡ä¿åº
    seen = set()
    uniq = []
    for x in out:
        if x not in seen:
            uniq.append(x)
            seen.add(x)
    return uniq


def should_refuse_by_score(docs_with_scores: List[Tuple[Document, float]], distance_threshold: float) -> bool:
    """
    Chroma çš„ similarity_search_with_score è¿”å›çš„ score å¸¸è§æ˜¯â€œè·ç¦»â€ï¼Œè¶Šå°è¶Šç›¸è¿‘ã€‚
    è‹¥æœ€ç›¸è¿‘çš„è·ç¦»ä»ç„¶å¾ˆå¤§ => æ£€ç´¢ä¸å¯é  => è§¦å‘æ‹’ç­”ã€‚
    """
    if not docs_with_scores:
        return True
    best = docs_with_scores[0][1]
    return best > distance_threshold


# =========================
# Streamlit UI
# =========================
st.title("ğŸ©º åŒ»ç–—é¢†åŸŸ RAG é—®ç­” Demoï¼ˆå¤šè½® + å¼•ç”¨ + æ‹’ç­”ï¼‰")

with st.sidebar:
    st.header("å‚æ•°")
    k = st.slider("æ£€ç´¢ Top-K", min_value=1, max_value=10, value=4, step=1)
    # è¿™ä¸ªé˜ˆå€¼éœ€è¦ä½ æ ¹æ® embedding/åº“åˆ†å¸ƒè°ƒä¸€ä¸‹ã€‚é»˜è®¤ç»™ä¸ªä¿å®ˆå€¼ã€‚
    distance_threshold = st.slider("æ‹’ç­”è·ç¦»é˜ˆå€¼ï¼ˆè¶Šå°è¶Šä¸¥æ ¼ï¼‰", min_value=0.1, max_value=2.0, value=0.8, step=0.05)
    max_turns = st.slider("ä¿ç•™æœ€è¿‘å¯¹è¯è½®æ•°", min_value=2, max_value=20, value=8, step=1)
    enable_query_rewrite = st.checkbox("å¯ç”¨å¤šè½®æ£€ç´¢æŸ¥è¯¢æ”¹å†™", value=True)
    st.divider()
    if st.button("ğŸ§¹ æ¸…ç©ºå¯¹è¯"):
        st.session_state.messages = []
        st.session_state.summary = ""
        st.session_state.last_sources = []
        st.rerun()

# ä¼šè¯çŠ¶æ€
if "messages" not in st.session_state:
    st.session_state.messages = []  # list of {"role": ..., "content": ...}
if "summary" not in st.session_state:
    st.session_state.summary = ""  # æ»šåŠ¨æ‘˜è¦
if "last_sources" not in st.session_state:
    st.session_state.last_sources = []  # æœ€è¿‘ä¸€æ¬¡æ£€ç´¢åˆ°çš„ docs_with_scores

# å±•ç¤ºå†å²æ¶ˆæ¯
for m in st.session_state.messages:
    with st.chat_message(m["role"]):
        st.markdown(m["content"])

# è¾“å…¥æ¡†
user_text = st.chat_input("è¯·è¾“å…¥åŒ»ç–—é—®é¢˜ï¼ˆæ”¯æŒå¤šè½®å¯¹è¯ï¼‰â€¦")

if user_text:
    # è®°å½•ç”¨æˆ·æ¶ˆæ¯
    st.session_state.messages.append({"role": "user", "content": user_text})
    with st.chat_message("user"):
        st.markdown(user_text)

    # ç”Ÿæˆ recent + summary
    recent_text = format_recent_messages(st.session_state.messages, max_turns=max_turns)
    summary_text = st.session_state.summary

    # 1) æŸ¥è¯¢æ”¹å†™ï¼ˆå¯é€‰ï¼‰
    if enable_query_rewrite:
        rewrite_chain = rewrite_prompt | model
        rewrite_resp = rewrite_chain.invoke({
            "summary": summary_text,
            "recent": recent_text,
            "question": user_text
        })
        query = rewrite_resp.content.strip() if hasattr(rewrite_resp, "content") else str(rewrite_resp).strip()
    else:
        query = user_text.strip()

    # 2) æ£€ç´¢ï¼ˆå¸¦ scoreï¼‰
    docs_with_scores = vector_store.similarity_search_with_score(query, k=k)
    st.session_state.last_sources = docs_with_scores
    context_text = docs_to_context_with_ids(docs_with_scores)

    # 3) æ‹’ç­”åˆ¤å®šï¼ˆæ£€ç´¢ä¸å¯é  -> æ‹’ç»ï¼‰
    refuse = should_refuse_by_score(docs_with_scores, distance_threshold=distance_threshold)

    # 4) ç”Ÿæˆå›ç­”ï¼ˆæµå¼ï¼‰
    with st.chat_message("assistant"):
        placeholder = st.empty()
        full_answer = ""

        if refuse:
            # ç›´æ¥æ‹’ç­”ï¼ˆä»ç»™å‡ºå®‰å…¨å»ºè®®ï¼‰
            full_answer = (
                "ã€å›ç­”ã€‘\n"
                "æˆ‘åœ¨å½“å‰çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ£€ç´¢åˆ°è¶³å¤Ÿå¯é çš„ä¾æ®æ¥å›ç­”è¿™ä¸ªé—®é¢˜ï¼ˆèµ„æ–™ç›¸ä¼¼åº¦ä¸è¶³ï¼‰ã€‚\n\n"
                "ä½ å¯ä»¥è¡¥å……ï¼šç—‡çŠ¶æŒç»­æ—¶é—´ã€æ˜¯å¦å‘çƒ­/å’³å—½ã€æ—¢å¾€ç—…å²ã€ç”¨è¯å²ã€æ£€æŸ¥ç»“æœç­‰ï¼›\n"
                "å¦‚æœå‡ºç°èƒ¸ç—›ã€å‘¼å¸å›°éš¾ã€æŒç»­é«˜çƒ­ã€æ„è¯†æ”¹å˜ç­‰å±é™©ä¿¡å·ï¼Œè¯·å°½å¿«å°±åŒ»ã€‚\n\n"
                "ã€å¼•ç”¨æ¥æºã€‘\n"
                "ï¼ˆæ— ï¼šæœ¬æ¬¡æ£€ç´¢ç»“æœä¸å¯é ï¼Œæœªå¼•ç”¨ï¼‰"
            )
            placeholder.markdown(full_answer)
        else:
            answer_chain = answer_prompt | model
            # Streamlit å®æ—¶è¾“å‡º
            for chunk in answer_chain.stream({
                "summary": summary_text,
                "recent": recent_text,
                "question": user_text,
                "context": context_text
            }):
                # chunk å¯èƒ½æ˜¯ AIMessageChunk
                part = chunk.content if hasattr(chunk, "content") else str(chunk)
                full_answer += part
                placeholder.markdown(full_answer)

    # 5) è®°å½•åŠ©æ‰‹æ¶ˆæ¯
    st.session_state.messages.append({"role": "assistant", "content": full_answer})

    # 6) æ›´æ–°æ»šåŠ¨æ‘˜è¦ï¼ˆå½“æ¶ˆæ¯å¾ˆé•¿æ—¶æŠŠæ—§å¯¹è¯å‹ç¼©è¿› summaryï¼‰
    # ç®€å•ç­–ç•¥ï¼šæ¯æ¬¡éƒ½ç”¨â€œæœ€è¿‘ä¸€æ®µå¯¹è¯â€æ›´æ–°æ‘˜è¦ï¼ˆä½ ä¹Ÿå¯æ”¹æˆæ¯ N è½®æ›´æ–°ä¸€æ¬¡ï¼‰
    try:
        # å–æœ€åå‡ æ¡ç”¨äºâ€œæ–°å¢ç‰‡æ®µâ€
        new_lines = format_recent_messages(st.session_state.messages, max_turns=min(max_turns, 6))
        sum_chain = summary_prompt | model
        sum_resp = sum_chain.invoke({"summary": st.session_state.summary, "new_lines": new_lines})
        st.session_state.summary = sum_resp.content.strip() if hasattr(sum_resp, "content") else str(sum_resp).strip()
    except Exception:
        # æ‘˜è¦å¤±è´¥ä¸å½±å“ä¸»æµç¨‹
        pass

    # 7) æ˜¾ç¤ºå¼•ç”¨æ¥æºï¼ˆUI å±•ç¤ºæ£€ç´¢åˆ°çš„èµ„æ–™ï¼‰
    st.divider()
    st.subheader("ğŸ“Œ æœ¬è½®æ£€ç´¢åˆ°çš„èµ„æ–™ï¼ˆå¯è§£é‡Šå¼•ç”¨ï¼‰")
    used_nums = extract_used_source_nums(full_answer)

    for idx, (doc, score) in enumerate(docs_with_scores, start=1):
        meta = doc.metadata or {}
        title = f"èµ„æ–™{idx} | answer_id={meta.get('answer_id')} | question_id={meta.get('question_id')} | score={score:.4f}"
        if idx in used_nums:
            title = "âœ… " + title

        with st.expander(title, expanded=(idx == 1)):
            st.markdown(doc.page_content)
            st.caption(f"metadata: {meta}")
